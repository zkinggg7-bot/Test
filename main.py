
import os
import json
import time
import threading
import requests
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
from datetime import datetime

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ==========================================
app = Flask(__name__)
CORS(app)

# Ù…ÙØªØ§Ø­ Ø³Ø±ÙŠ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø·
API_SECRET = os.environ.get('API_SECRET', 'Zeusndndjddnejdjdjdejekk29393838msmskxcm9239484jdndjdnddjj99292938338zeuslojdnejxxmejj82283849')

# Ø±Ø§Ø¨Ø· Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Node.js)
NODE_BACKEND_URL = os.environ.get('NODE_BACKEND_URL', 'https://c-production-3db6.up.railway.app')

# ==========================================
# Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© (Shared Scraper Tools)
# ==========================================

def get_headers(custom_user_agent=None):
    """
    ØªÙƒÙˆÙŠÙ† Ø§Ù„ØªØ±ÙˆÙŠØ³Ø© (Headers).
    Ø¥Ø°Ø§ ØªÙ… ØªÙ…Ø±ÙŠØ± User-Agent Ù…Ù† Ø§Ù„Ø¢ÙŠØ¨Ø§Ø¯ØŒ Ù†Ø³ØªØ®Ø¯Ù…Ù‡ Ù‡Ùˆ Ø­ØµØ±Ø§Ù‹ Ù„Ù†Ø·Ø§Ø¨Ù‚ Ø§Ù„Ø¬Ù„Ø³Ø©.
    """
    headers = {
        'User-Agent': custom_user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
        # Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ ØªØªØ·Ù„Ø¨ Referer
        'Referer': 'https://google.com/' 
    }
    return headers

def fix_image_url(url, base_url='https://api.rewayat.club'):
    if not url: return ""
    if url.startswith('//'):
        return 'https:' + url
    elif url.startswith('/'):
        return base_url + url
    elif not url.startswith('http'):
        return base_url + '/' + url
    return url

def send_data_to_backend(payload):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    try:
        endpoint = f"{NODE_BACKEND_URL}/api/scraper/receive"
        headers = { 'Content-Type': 'application/json', 'Authorization': API_SECRET, 'x-api-secret': API_SECRET }
        response = requests.post(endpoint, json=payload, headers=headers, timeout=60)
        return response.status_code == 200
    except Exception as e:
        print(f"âŒ Failed to send data: {e}")
        return False

def check_existing_chapters(title):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    try:
        endpoint = f"{NODE_BACKEND_URL}/api/scraper/check-chapters"
        headers = { 'Content-Type': 'application/json', 'Authorization': API_SECRET, 'x-api-secret': API_SECRET }
        response = requests.post(endpoint, json={'title': title}, headers=headers, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('exists'):
                return data['chapters'] # ÙŠØ¹ÙŠØ¯ Ù…ØµÙÙˆÙØ© Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
            else:
                return []
        return []
    except Exception as e:
        print(f"âŒ Error checking existence: {e}")
        return []

# ==========================================
# ğŸŸ£ 1. Rewayat Club (Nuxt) Logic - Probe Mode
# ==========================================

def extract_from_nuxt(soup):
    try:
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'window.__NUXT__' in script.string:
                content = script.string
                match = re.search(r'poster_url:"(.*?)"', content)
                if not match: match = re.search(r'poster:"(.*?)"', content)
                if match:
                    raw_url = match.group(1)
                    return raw_url.encode('utf-8').decode('unicode_escape')
    except: pass
    return None

def fetch_metadata_rewayat(url, cookies=None, user_agent=None):
    try:
        # ğŸ”¥ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙˆÙƒÙŠØ² ÙˆØ§Ù„Ù€ User-Agent Ø§Ù„Ù‚Ø§Ø¯Ù… Ù…Ù† Ø§Ù„Ø¢ÙŠØ¨Ø§Ø¯
        print(f"Fetching Metadata with UA: {user_agent}")
        response = requests.get(url, headers=get_headers(user_agent), cookies=cookies, timeout=15)
        
        if response.status_code != 200: 
            print(f"Failed to fetch metadata. Status: {response.status_code}")
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title_tag = soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else "Unknown Title"
        
        cover_url = extract_from_nuxt(soup) or ""
        if not cover_url:
            og_image = soup.find("meta", property="og:image")
            if og_image: cover_url = og_image["content"]
        cover_url = fix_image_url(cover_url)

        desc_div = soup.find(class_='text-pre-line') or soup.find('div', class_='v-card__text')
        description = desc_div.get_text(strip=True) if desc_div else ""
        
        return {
            'title': title, 'description': description, 'cover': cover_url,
            'status': "Ù…Ø³ØªÙ…Ø±Ø©", 'category': "Ø¹Ø§Ù…", 'tags': []
        }
    except Exception as e:
        print(f"Error rewayat metadata: {e}")
        return None

def scrape_chapter_rewayat(novel_url, chapter_num, cookies=None, user_agent=None):
    url = f"{novel_url.rstrip('/')}/{chapter_num}"
    try:
        # ğŸ”¥ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¬Ù„Ø³Ø© Ù„ÙƒÙ„ ÙØµÙ„
        response = requests.get(url, headers=get_headers(user_agent), cookies=cookies, timeout=10)
        if response.status_code != 200: return None, None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        paragraphs = soup.find_all('p')
        clean_paragraphs = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
        if clean_paragraphs:
            text = "\n\n".join(clean_paragraphs)
        else:
            div = soup.find('div', class_='pre-formatted') or soup.find('div', class_='v-card__text')
            text = div.get_text(separator="\n\n", strip=True) if div else ""
        
        if len(text.strip()) < 2: return None, None
        
        title_tag = soup.find(class_='v-card__subtitle') or soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else f"Ø§Ù„ÙØµÙ„ {chapter_num}"
        title = re.sub(r'^\d+\s*-\s*', '', title)
        return title, text
    except: return None, None

def worker_rewayat_probe(url, admin_email, metadata, cookies=None, user_agent=None):
    existing_chapters = check_existing_chapters(metadata['title'])
    skip_meta = len(existing_chapters) > 0
    
    if not skip_meta:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': [], 'skipMetadataUpdate': False})

    current_chapter = 1
    errors = 0
    batch = []
    
    while current_chapter < 5000 and errors < 15:
        if current_chapter in existing_chapters:
            current_chapter += 1
            errors = 0
            continue
            
        chap_title, content = scrape_chapter_rewayat(url, current_chapter, cookies, user_agent)
        if content:
            errors = 0
            batch.append({'number': current_chapter, 'title': chap_title, 'content': content})
            print(f"Fetched Ch {current_chapter}")
            if len(batch) >= 5:
                send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})
                batch = []
                time.sleep(1) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        else:
            errors += 1
            print(f"Failed Ch {current_chapter} ({errors}/15)")
        current_chapter += 1
        
    if batch:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})

# ==========================================
# ğŸŸ¢ 2. Ar Novel (Madara) Logic - List Mode
# ==========================================

def fetch_metadata_madara(url, cookies=None, user_agent=None):
    try:
        # ğŸ”¥ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¬Ù„Ø³Ø©
        response = requests.get(url, headers=get_headers(user_agent), cookies=cookies, timeout=15)
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title_tag = soup.find(class_='post-title')
        title = title_tag.find('h1').get_text(strip=True) if title_tag else "Unknown"
        title = re.sub(r'\s*~.*$', '', title) 

        cover = ""
        img_container = soup.find(class_='summary_image')
        if img_container:
            img_tag = img_container.find('img')
            if img_tag:
                cover = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('srcset', '').split(' ')[0]
        
        if not cover:
            og_img = soup.find("meta", property="og:image")
            if og_img: cover = og_img["content"]

        # Madara ID Extraction
        novel_id = None
        shortlink = soup.find("link", rel="shortlink")
        if shortlink:
            match = re.search(r'p=(\d+)', shortlink.get('href', ''))
            if match: novel_id = match.group(1)
            
        if not novel_id:
            id_input = soup.find('input', class_='rating-post-id')
            if id_input: novel_id = id_input.get('value')
            
        if not novel_id:
            body_class = soup.find('body').get('class', [])
            for c in body_class:
                if c.startswith('manga-id-'):
                    novel_id = c.replace('manga-id-', '')

        desc_div = soup.find(class_='summary__content') or soup.find(class_='description-summary')
        description = desc_div.get_text(separator="\n", strip=True) if desc_div else ""

        genres_content = soup.find(class_='genres-content')
        category = "Ø¹Ø§Ù…"
        tags = []
        if genres_content:
            links = genres_content.find_all('a')
            tags = [a.get_text(strip=True) for a in links]
            if tags: category = tags[0]

        return {
            'title': title, 'description': description, 'cover': cover,
            'status': 'Ù…Ø³ØªÙ…Ø±Ø©', 'category': category, 'tags': tags,
            'novel_id': novel_id
        }
    except Exception as e:
        print(f"Error Madara Meta: {e}")
        return None

def parse_madara_chapters_from_html(soup):
    chapters = []
    items = soup.find_all('li', class_='wp-manga-chapter')
    
    for item in items:
        a = item.find('a')
        if a:
            link = a.get('href')
            raw_title = a.get_text(strip=True)
            num_match = re.search(r'(\d+)', raw_title)
            number = int(num_match.group(1)) if num_match else 0
            clean_title = re.sub(r'^\d+\s*[-â€“]\s*', '', raw_title).strip()
            
            if number > 0:
                chapters.append({'number': number, 'url': link, 'title': clean_title})
    
    return chapters

def fetch_chapter_list_madara(novel_id, novel_url=None, cookies=None, user_agent=None):
    chapters = []
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© 1: AJAX
    if novel_url:
        ajax_endpoint = f"{novel_url.rstrip('/')}/ajax/chapters/"
        try:
            headers = get_headers(user_agent)
            headers['X-Requested-With'] = 'XMLHttpRequest'
            # ğŸ”¥ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¬Ù„Ø³Ø© ÙÙŠ Ø·Ù„Ø¨ Ø§Ù„Ù€ AJAX
            res = requests.post(ajax_endpoint, headers=headers, cookies=cookies, timeout=20)
            if res.status_code == 200:
                soup = BeautifulSoup(res.content, 'html.parser')
                chapters = parse_madara_chapters_from_html(soup)
        except Exception as e:
            print(f"AJAX endpoint failed: {e}")

    # Ù…Ø­Ø§ÙˆÙ„Ø© 2: admin-ajax (Fallback)
    if not chapters and novel_id:
        try:
            ajax_url = "https://ar-no.com/wp-admin/admin-ajax.php"
            data = {'action': 'manga_get_chapters', 'manga': novel_id}
            # ğŸ”¥ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¬Ù„Ø³Ø©
            res = requests.post(ajax_url, data=data, headers=get_headers(user_agent), cookies=cookies, timeout=20)
            if res.status_code == 200:
                soup = BeautifulSoup(res.content, 'html.parser')
                chapters = parse_madara_chapters_from_html(soup)
        except: pass
            
    if chapters:
        chapters.sort(key=lambda x: x['number'])
    
    return chapters

def scrape_chapter_madara(url, cookies=None, user_agent=None):
    try:
        # ğŸ”¥ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¬Ù„Ø³Ø©
        res = requests.get(url, headers=get_headers(user_agent), cookies=cookies, timeout=15)
        if res.status_code != 200: return None
        soup = BeautifulSoup(res.content, 'html.parser')
        
        container = soup.find(class_='text-left') or soup.find(class_='reading-content') or soup.find(class_='entry-content')
            
        if container:
            for bad in container.find_all(['div', 'script', 'style', 'input', 'ins', 'iframe']):
                if bad.get('class') and any(c in ['code-block', 'adsbygoogle', 'pf-ad'] for c in bad.get('class')):
                    bad.decompose()
            
            for nav in container.find_all('div', class_='nav-links'):
                nav.decompose()

            text = container.get_text(separator="\n\n", strip=True)
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = text.replace('Ø§ÙƒÙ…Ø§Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©', '')
            return text
        return None
    except: return None

def worker_madara_list(url, admin_email, metadata, cookies=None, user_agent=None):
    existing_chapters = check_existing_chapters(metadata['title'])
    skip_meta = len(existing_chapters) > 0
    
    if not skip_meta:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': [], 'skipMetadataUpdate': False})

    all_chapters = fetch_chapter_list_madara(metadata.get('novel_id'), url, cookies, user_agent)
    
    if not all_chapters:
        print(f"No chapters found for {metadata['title']}")
        return

    print(f"Processing {len(all_chapters)} chapters.")
    
    batch = []
    for chap in all_chapters:
        if chap['number'] in existing_chapters:
            continue
            
        print(f"Scraping {metadata['title']} - Ch {chap['number']}...")
        content = scrape_chapter_madara(chap['url'], cookies, user_agent)
        
        if content:
            batch.append({
                'number': chap['number'],
                'title': chap['title'],
                'content': content
            })
            
            if len(batch) >= 5:
                send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})
                batch = []
                time.sleep(1.2)
        
    if batch:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})

# ==========================================
# Main Orchestrator
# ==========================================

@app.route('/', methods=['GET'])
def health_check():
    return "ZEUS Scraper Service is Running", 200

# Endpoint Ø§Ù„Ù‚Ø¯ÙŠÙ… (Ù„Ù„Ø³Ø­Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¨Ø¯ÙˆÙ† ÙƒÙˆÙƒÙŠØ²)
@app.route('/scrape', methods=['POST'])
def trigger_scrape():
    auth_header = request.headers.get('Authorization')
    if auth_header != API_SECRET: return jsonify({'message': 'Unauthorized'}), 401

    data = request.json
    url = data.get('url', '')
    admin_email = data.get('adminEmail')
    
    if not url: return jsonify({'message': 'No URL'}), 400

    return start_scraping_logic(url, admin_email, None, None)

# ğŸ”¥ Endpoint Ø§Ù„Ø¬Ø¯ÙŠØ¯ (Ù„Ù„Ø³Ø­Ø¨ Ø§Ù„Ù…Ø­Ù…ÙŠ Ù…Ø¹ Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ø¢ÙŠØ¨Ø§Ø¯)
@app.route('/scrape-protected', methods=['POST'])
def trigger_protected_scrape():
    auth_header = request.headers.get('Authorization')
    if auth_header != API_SECRET: return jsonify({'message': 'Unauthorized'}), 401

    data = request.json
    url = data.get('url', '')
    admin_email = data.get('adminEmail')
    cookies = data.get('cookies') # ğŸ”¥ Dictionary of Cookies
    user_agent = data.get('userAgent') # ğŸ”¥ User Agent
    
    if not url: return jsonify({'message': 'No URL'}), 400

    print(f"Received Protected Scrape Request for: {url}")
    return start_scraping_logic(url, admin_email, cookies, user_agent)

def start_scraping_logic(url, admin_email, cookies, user_agent):
    if 'rewayat.club' in url:
        # ØªÙ…Ø±ÙŠØ± Ø§Ù„ÙƒÙˆÙƒÙŠØ² Ù„Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
        meta = fetch_metadata_rewayat(url, cookies, user_agent)
        if not meta: return jsonify({'message': 'Failed metadata'}), 400
        
        # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø§Ù…Ù„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ© Ù…Ø¹ Ø§Ù„ÙƒÙˆÙƒÙŠØ²
        thread = threading.Thread(target=worker_rewayat_probe, args=(url, admin_email, meta, cookies, user_agent))
        thread.daemon = False 
        thread.start()
        return jsonify({'message': 'Protected Scraping started in background.'}), 200
        
    elif 'ar-no.com' in url:
        meta = fetch_metadata_madara(url, cookies, user_agent)
        if not meta: return jsonify({'message': 'Failed metadata'}), 400
        
        thread = threading.Thread(target=worker_madara_list, args=(url, admin_email, meta, cookies, user_agent))
        thread.daemon = False
        thread.start()
        return jsonify({'message': 'Protected Scraping started in background.'}), 200

    else:
        return jsonify({'message': 'Unsupported Domain'}), 400

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, threaded=True)
