import os
import json
import time
import threading
import requests
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
from datetime import datetime

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ==========================================
app = Flask(__name__)
CORS(app)

# Ù…ÙØªØ§Ø­ Ø³Ø±ÙŠ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø·
API_SECRET = os.environ.get('API_SECRET', 'Zeusndndjddnejdjdjdejekk29393838msmskxcm9239484jdndjdnddjj99292938338zeuslojdnejxxmejj82283849')

# Ø±Ø§Ø¨Ø· Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Node.js)
NODE_BACKEND_URL = os.environ.get('NODE_BACKEND_URL', 'https://c-production-3db6.up.railway.app')

# ==========================================
# Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø³Ø­Ø¨ (Scraper Tools)
# ==========================================

def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3'
    }

def extract_from_nuxt(soup):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Nuxt Ø§Ù„Ø®Ø§Ù…"""
    try:
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'window.__NUXT__' in script.string:
                content = script.string
                match = re.search(r'poster_url:"(.*?)"', content)
                if not match:
                    match = re.search(r'poster:"(.*?)"', content)
                
                if match:
                    raw_url = match.group(1)
                    clean_url = raw_url.encode('utf-8').decode('unicode_escape')
                    return clean_url
    except Exception as e:
        print(f"Error extracting from Nuxt: {e}")
    return None

def extract_background_image(style_str):
    if not style_str: return ''
    clean_style = style_str.replace('&quot;', '"').replace("&#39;", "'")
    match = re.search(r'url\s*\((.*?)\)', clean_style, re.IGNORECASE)
    if match:
        url = match.group(1).strip()
        url = url.strip('"\'')
        return url
    return ''

def is_valid_tag(text):
    text = text.strip()
    if not text: return False
    if text in ['Ù…ÙƒØªÙ…Ù„Ø©', 'Ù…ØªÙˆÙ‚ÙØ©', 'Ù…Ø³ØªÙ…Ø±Ø©', 'Ù…ØªØ±Ø¬Ù…Ø©', 'Ø±ÙˆØ§ÙŠØ©', 'Ø¹Ù…Ù„']: return False
    clean_text = text.replace(',', '').replace('.', '').replace('x', '').strip()
    if clean_text.isdigit(): return False
    if re.search(r'^\d+\s*x$', text, re.IGNORECASE): return False 
    if not re.search(r'[\u0600-\u06FF]', text): return False
    return True

def fix_image_url(url):
    if not url: return ""
    base_api_url = 'https://api.rewayat.club'
    if url.startswith('//'):
        return 'https:' + url
    elif url.startswith('/'):
        return base_api_url + url
    elif not url.startswith('http'):
        return base_api_url + '/' + url
    return url

def fetch_novel_metadata_html(url):
    """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø±ÙˆØ§ÙŠØ©"""
    try:
        print(f"ğŸ“¡ Fetching metadata from HTML: {url}")
        response = requests.get(url, headers=get_headers(), timeout=15)
        if response.status_code != 200:
            print(f"âŒ HTTP Error: {response.status_code}")
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Title
        title_tag = soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else "Unknown Title"
        
        # Cover
        cover_url = ""
        nuxt_image = extract_from_nuxt(soup)
        if nuxt_image: cover_url = nuxt_image
        
        if not cover_url:
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"): cover_url = og_image["content"]
        
        if not cover_url:
            img_div = soup.find('div', class_='v-image__image--cover')
            if img_div and img_div.has_attr('style'): cover_url = extract_background_image(img_div['style'])
        
        cover_url = fix_image_url(cover_url)

        # Description
        desc_div = soup.find(class_='text-pre-line') or soup.find('div', class_='v-card__text')
        description = desc_div.get_text(strip=True) if desc_div else ""
        
        # Status & Category
        status = "Ù…Ø³ØªÙ…Ø±Ø©"
        tags = []
        category = "Ø¹Ø§Ù…"
        
        chip_groups = soup.find_all(class_='v-chip-group')
        target_chips = []
        if chip_groups:
            for group in chip_groups[:2]: 
                target_chips.extend(group.find_all(class_='v-chip__content'))
        else:
            target_chips = soup.find_all(class_='v-chip__content')

        for chip in target_chips:
            text = chip.get_text(strip=True)
            if text in ['Ù…ÙƒØªÙ…Ù„Ø©', 'Ù…ØªÙˆÙ‚ÙØ©', 'Ù…Ø³ØªÙ…Ø±Ø©']:
                status = text
            elif is_valid_tag(text):
                tags.append(text)
        
        tags = list(set(tags))
        if tags: category = tags[0]

        # Total Chapters (ØªÙ‚Ø±ÙŠØ¨ÙŠ)
        total_chapters = 0
        all_text = soup.get_text()
        chapter_match = re.search(r'Ø§Ù„ÙØµÙˆÙ„\s*\((\d+)\)', all_text)
        if chapter_match:
            total_chapters = int(chapter_match.group(1))
        
        if total_chapters == 0:
            tabs = soup.find_all(class_='v-tab')
            for tab in tabs:
                if "Ø§Ù„ÙØµÙˆÙ„" in tab.get_text():
                    match = re.search(r'(\d+)', tab.get_text())
                    if match: total_chapters = int(match.group(1))

        return {
            'title': title,
            'description': description,
            'cover': cover_url,
            'status': status,
            'tags': tags,
            'category': category,
            'total_chapters': total_chapters
        }

    except Exception as e:
        print(f"âŒ Error scraping metadata: {e}")
        return None

def scrape_chapter_content_html(novel_url, chapter_num):
    """Ø³Ø­Ø¨ Ù†Øµ Ø§Ù„ÙØµÙ„"""
    url = f"{novel_url.rstrip('/')}/{chapter_num}"
    try:
        response = requests.get(url, headers=get_headers(), timeout=10)
        if response.status_code != 200:
            return None, None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        paragraphs = soup.find_all('p')
        
        # === ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§: Ø¥Ø²Ø§Ù„Ø© Ø´Ø±Ø· Ø§Ù„Ø·ÙˆÙ„ (> 20) Ù„Ø³Ø­Ø¨ ÙƒÙ„ Ø´ÙŠØ¡ ===
        # Ø§Ù„Ø¢Ù† Ù†ØªØ­Ù‚Ù‚ ÙÙ‚Ø· Ù…Ù† Ø£Ù† Ø§Ù„ÙÙ‚Ø±Ø© Ù„ÙŠØ³Øª ÙØ§Ø±ØºØ© ØªÙ…Ø§Ù…Ø§Ù‹
        clean_paragraphs = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
        
        if clean_paragraphs:
            text_content = "\n\n".join(clean_paragraphs)
        else:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¯ÙŠÙ„Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ ÙˆØ³ÙˆÙ… <p>
            content_div = soup.find('div', class_='pre-formatted') or soup.find('div', class_='v-card__text')
            if content_div:
                text_content = content_div.get_text(separator="\n\n", strip=True)
            else:
                return None, None
            
        # === ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§: ØªØ®ÙÙŠÙ Ø´Ø±Ø· Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø·ÙˆÙ„ Ø§Ù„ÙØµÙ„ ===
        # ÙƒØ§Ù† < 50 Ø³Ø§Ø¨Ù‚Ø§Ù‹ØŒ Ø¬Ø¹Ù„Ù†Ø§Ù‡ < 2 Ù„ÙŠØ³Ù…Ø­ Ø¨Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª
        if len(text_content.strip()) < 2:
            return None, None

        title_tag = soup.find(class_='v-card__subtitle') or soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else f"Ø§Ù„ÙØµÙ„ {chapter_num}"
        title = re.sub(r'^\d+\s*-\s*', '', title)

        return title, text_content
            
    except Exception as e:
        print(f"Error scraping chapter {chapter_num}: {e}")
        return None, None

def check_existing_chapters(title):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯"""
    try:
        endpoint = f"{NODE_BACKEND_URL}/api/scraper/check-chapters"
        headers = { 'Content-Type': 'application/json', 'Authorization': API_SECRET, 'x-api-secret': API_SECRET }
        response = requests.post(endpoint, json={'title': title}, headers=headers, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('exists'):
                return data['chapters'] # ÙŠØ¹ÙŠØ¯ Ù…ØµÙÙˆÙØ© Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
            else:
                return []
        return []
    except Exception as e:
        print(f"âŒ Error checking existence: {e}")
        return []

def send_data_to_backend(payload):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    try:
        endpoint = f"{NODE_BACKEND_URL}/api/scraper/receive"
        headers = { 'Content-Type': 'application/json', 'Authorization': API_SECRET, 'x-api-secret': API_SECRET }
        response = requests.post(endpoint, json=payload, headers=headers, timeout=60)
        return response.status_code == 200
    except Exception as e:
        print(f"âŒ Failed to send data: {e}")
        return False

# ==========================================
# Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ù„ÙÙŠØ© (ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ù„ØªØ¹Ù…Ù„ Ø¨ÙˆØ¶Ø¹ Probe Mode)
# ==========================================
def background_worker(url, admin_email, author_name, start_from=1, update_info=True):
    print(f"ğŸš€ Starting Scraper for: {url}")
    
    # 1. Ø§Ù„Ù…ÙŠØªØ§Ø¯Ø§ØªØ§
    metadata = fetch_novel_metadata_html(url)
    if not metadata:
        send_data_to_backend({'adminEmail': admin_email, 'error': 'ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ÙˆØ§ÙŠØ©'})
        return

    print(f"ğŸ“– Found Novel: {metadata['title']} (Site says approx: {metadata['total_chapters']} Chaps)")

    # 2. Ø§Ù„Ù…ØµØ§ÙØ­Ø© Ø§Ù„Ø°ÙƒÙŠØ©
    existing_chapters = check_existing_chapters(metadata['title'])
    skip_metadata_update = False
    
    max_existing = 0
    if len(existing_chapters) > 0:
        max_existing = max(existing_chapters)
        print(f"â„¹ï¸ Novel exists locally with {len(existing_chapters)} chapters. Max ID: {max_existing}")
        skip_metadata_update = True
    else:
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ø¯ÙŠØ¯Ø© ÙƒÙ„ÙŠØ§Ù‹
        if not send_data_to_backend({
            'adminEmail': admin_email, 'novelData': metadata, 'chapters': [], 'skipMetadataUpdate': False
        }): return

    # 3. Ø­Ù„Ù‚Ø© Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ø°ÙƒÙŠØ© (While Loop for Probing)
    # Ù„Ø§ Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ range Ø«Ø§Ø¨ØªØŒ Ø¨Ù„ Ù†Ø³ØªÙ…Ø± Ø­ØªÙ‰ Ù†ØµÙ„ Ù„Ø­Ø¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ©
    
    current_chapter = start_from
    consecutive_errors = 0
    MAX_CONSECUTIVE_ERRORS = 15 # Ø³ÙŠØªÙˆÙ‚Ù Ø¨Ø¹Ø¯ 15 Ù…Ø­Ø§ÙˆÙ„Ø© ÙØ§Ø´Ù„Ø© Ù…ØªØªØ§Ù„ÙŠØ© (Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ÙØµÙˆÙ„ Ø¬Ø¯ÙŠØ¯Ø©)
    
    # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù„Ù„Ø£Ù…Ø§Ù† Ù„Ù…Ù†Ø¹ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù„Ø§Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ Ø­Ø§Ù„ ØªØ¹Ø·Ù„ Ø§Ù„Ù…ÙˆÙ‚Ø¹
    SAFETY_LIMIT = 5000 
    
    batch_size = 5 
    current_batch = []
    
    print(f"ğŸ•µï¸â€â™‚ï¸ Starting PROBE MODE from chapter {current_chapter}...")

    while current_chapter < SAFETY_LIMIT:
        
        # Ø£) Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙØµÙ„ Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ Ù…Ø³Ø¨Ù‚Ø§Ù‹
        if current_chapter in existing_chapters:
            # Ù†ØªØ®Ø·Ù‰ Ø§Ù„ÙØµÙ„ØŒ Ù„ÙƒÙ† Ù†ØµÙØ± Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ù„Ø£Ù† Ø§Ù„ØªØ³Ù„Ø³Ù„ ØµØ­ÙŠØ­ ÙˆÙ…ÙˆØ¬ÙˆØ¯
            consecutive_errors = 0
            if current_chapter % 50 == 0:
                print(f"â© Skipped existing chapter {current_chapter}...")
            current_chapter += 1
            continue
        
        # Ø¨) Ø§Ù„ÙØµÙ„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù†Ø­Ø§ÙˆÙ„ Ø³Ø­Ø¨Ù‡
        chap_title, content = scrape_chapter_content_html(url, current_chapter)
        
        if content:
            # Ù†Ø¬Ø§Ø­!
            consecutive_errors = 0 # ØªØµÙÙŠØ± Ø§Ù„Ø¹Ø¯Ø§Ø¯
            
            chapter_data = {'number': current_chapter, 'title': chap_title, 'content': content}
            current_batch.append(chapter_data)
            print(f"ğŸ“„ Scraped NEW Chapter {current_chapter}")
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¯ÙØ¹Ø©
            if len(current_batch) >= batch_size:
                print(f"ğŸ“¤ Sending batch of {len(current_batch)}...")
                send_data_to_backend({
                    'adminEmail': admin_email, 'novelData': metadata, 'chapters': current_batch, 'skipMetadataUpdate': skip_metadata_update
                })
                current_batch = [] 
                time.sleep(1) 
        else:
            # ÙØ´Ù„ (404 Ø£Ùˆ Ù…Ø­ØªÙˆÙ‰ ÙØ§Ø±Øº)
            consecutive_errors += 1
            print(f"âš ï¸ Failed/Empty Ch {current_chapter} (Error {consecutive_errors}/{MAX_CONSECUTIVE_ERRORS})")
            
            # Ø´Ø±Ø· Ø§Ù„ØªÙˆÙ‚Ù Ø§Ù„Ø­Ø§Ø³Ù…
            if consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
                print(f"ğŸ›‘ Reached {MAX_CONSECUTIVE_ERRORS} consecutive errors at chapter {current_chapter}. Stopping probe.")
                break
        
        current_chapter += 1

    # Ø¥Ø±Ø³Ø§Ù„ Ù…Ø§ ØªØ¨Ù‚Ù‰ ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
    if len(current_batch) > 0:
        send_data_to_backend({
            'adminEmail': admin_email, 'novelData': metadata, 'chapters': current_batch, 'skipMetadataUpdate': skip_metadata_update
        })

    print(f"âœ¨ Scraping Task Completed! Scanned up to {current_chapter}")

# ==========================================
# Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoints)
# ==========================================

@app.route('/', methods=['GET'])
def health_check():
    return "ZEUS Scraper Service (Probe Mode Active ğŸ•µï¸â€â™‚ï¸) is Running âš¡", 200

@app.route('/scrape', methods=['POST'])
def trigger_scrape():
    auth_header = request.headers.get('Authorization')
    if auth_header != API_SECRET:
        return jsonify({'message': 'Unauthorized'}), 401

    data = request.json
    if not data:
        return jsonify({'message': 'No data provided'}), 400
        
    url = data.get('url')
    admin_email = data.get('adminEmail')
    author_name = data.get('authorName', 'ZEUS Bot')
    start_from = int(data.get('startFrom', 1))

    if not url or 'rewayat.club' not in url:
        return jsonify({'message': 'Invalid URL'}), 400

    thread = threading.Thread(target=background_worker, args=(url, admin_email, author_name, start_from))
    thread.daemon = True 
    thread.start()

    return jsonify({'message': 'Started', 'status': 'started'}), 200

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
