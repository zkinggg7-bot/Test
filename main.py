import os
import json
import time
import threading
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore
from pymongo import MongoClient
import certifi
from datetime import datetime
import re

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ==========================================
app = Flask(__name__)
CORS(app)

# Ù…ÙØªØ§Ø­ Ø³Ø±ÙŠ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø·
API_SECRET = os.environ.get('API_SECRET', 'Zeusndndjddnejdjdjdejekk29393838msmskxcm9239484jdndjdnddjj99292938338zeuslojdnejxxmejj82283849')

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================================

# 1. MongoDB Setup
MONGO_URI = os.environ.get('MONGODB_URI')
if MONGO_URI:
    try:
        mongo_client = MongoClient(MONGO_URI, tlsCAFile=certifi.where())
        mongo_db = mongo_client['zeus'] 
        novels_collection = mongo_db['novels']
        print("âœ… Connected to MongoDB")
    except Exception as e:
        print(f"âŒ MongoDB Connection Error: {e}")
else:
    print("âš ï¸ MONGODB_URI not found in env vars")

# 2. Firebase Setup
FIREBASE_KEY = os.environ.get('FIREBASE_SERVICE_ACCOUNT')
if FIREBASE_KEY:
    try:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ø´Ø§Ù…Ù„
        firebase_key_cleaned = FIREBASE_KEY.strip()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ BOM Ø£Ùˆ Ø£Ø­Ø±Ù Ø®ÙÙŠØ©
        firebase_key_cleaned = firebase_key_cleaned.encode('utf-8').decode('utf-8-sig')
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ JSON
        cred_dict = json.loads(firebase_key_cleaned)
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø®Ø§Øµ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚
        if 'private_key' in cred_dict:
            private_key = cred_dict['private_key']
            
            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ \\n Ø¨Ø³Ø·ÙˆØ± Ø­Ù‚ÙŠÙ‚ÙŠØ©
            private_key = private_key.replace('\\n', '\n')
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„ØªØ§Ø¨Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù…Ù† ÙƒÙ„ Ø³Ø·Ø±
            lines = private_key.split('\n')
            cleaned_lines = []
            for line in lines:
                # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ BEGIN/END ÙƒÙ…Ø§ Ù‡ÙŠ
                if '-----BEGIN' in line or '-----END' in line:
                    cleaned_lines.append(line.strip())
                else:
                    # Ø¥Ø²Ø§Ù„Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ù…Ù† Ø³Ø·ÙˆØ± Base64
                    cleaned_line = line.strip().replace(' ', '').replace('\t', '')
                    if cleaned_line:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ©
                        cleaned_lines.append(cleaned_line)
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙØªØ§Ø­
            cred_dict['private_key'] = '\n'.join(cleaned_lines)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„ØªÙ†Ø³ÙŠÙ‚
            if not cred_dict['private_key'].startswith('-----BEGIN PRIVATE KEY-----'):
                raise ValueError("Invalid private key format: missing BEGIN header")
            if not cred_dict['private_key'].endswith('-----END PRIVATE KEY-----'):
                raise ValueError("Invalid private key format: missing END footer")
        
        # ØªÙ‡ÙŠØ¦Ø© Firebase
        cred = credentials.Certificate(cred_dict)
        firebase_admin.initialize_app(cred)
        firestore_db = firestore.client()
        print("âœ… Connected to Firebase Firestore")
        
    except json.JSONDecodeError as e:
        print(f"âŒ Firebase JSON Parse Error: {e}")
        print(f"First 100 chars of FIREBASE_KEY: {FIREBASE_KEY[:100]}")
    except ValueError as e:
        print(f"âŒ Firebase Key Format Error: {e}")
    except Exception as e:
        print(f"âŒ Firebase Connection Error: {e}")
        import traceback
        print(traceback.format_exc())
else:
    print("âš ï¸ FIREBASE_SERVICE_ACCOUNT not found in env vars")

# ==========================================
# Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³Ø­Ø¨ (Scraper Logic)
# ==========================================

def get_slug_from_url(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±Ù Ø§Ù„ÙØ±ÙŠØ¯ Ù„Ù„Ø±ÙˆØ§ÙŠØ© Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    parts = url.split('/novel/')
    if len(parts) > 1:
        return parts[1].strip('/').split('/')[0]
    return None

def fetch_novel_metadata(slug):
    """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø±ÙˆØ§ÙŠØ© Ù…Ù† API Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
    api_url = f"https://api.rewayat.club/api/novel/{slug}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(api_url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            return {
                'title': data.get('arabic', data.get('english', 'Unknown')),
                'description': data.get('about', ''),
                'cover': f"https://api.rewayat.club{data.get('poster_url', '')}" if data.get('poster_url') else '',
                'status': 'Ù…ÙƒØªÙ…Ù„Ø©' if data.get('get_novel_status') == 'Ù…ÙƒØªÙ…Ù„Ø©' else 'Ù…Ø³ØªÙ…Ø±Ø©',
                'tags': [g['arabic'] for g in data.get('genre', [])],
                'slug': slug
            }
    except Exception as e:
        print(f"Error fetching metadata: {e}")
    return None

def fetch_all_chapters_list(slug):
    """Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØµÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±Ù‚ÙŠÙ…"""
    chapters = []
    page = 1
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    while True:
        url = f"https://api.rewayat.club/api/chapters/{slug}/?ordering=number&page={page}"
        try:
            print(f"Fetching chapters list page {page}...")
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                break
            
            data = response.json()
            results = data.get('results', [])
            
            if not results:
                break
                
            for item in results:
                chapters.append({
                    'number': item.get('number'),
                    'title': item.get('title'),
                    'id': item.get('id')
                })
            
            if not data.get('next'):
                break
                
            page += 1
            time.sleep(0.5)
            
        except Exception as e:
            print(f"Error fetching chapters list: {e}")
            break
            
    return chapters

def scrape_chapter_content(slug, chapter_num):
    """Ø³Ø­Ø¨ Ù†Øµ Ø§Ù„ÙØµÙ„ Ù…Ù† ØµÙØ­Ø© HTML"""
    url = f"https://rewayat.club/novel/{slug}/{chapter_num}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            content_div = soup.find('div', class_='content-area')
            
            if not content_div:
                content_div = soup.find('div', class_=lambda x: x and 'unselectable' in x)
            
            if content_div:
                paragraphs = content_div.find_all('p')
                text_content = "\n\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                return text_content
            
            paragraphs = soup.find_all('p')
            clean_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            return clean_text
            
    except Exception as e:
        print(f"Error scraping chapter {chapter_num}: {e}")
    
    return None

def background_worker(url, admin_email, author_name):
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    print(f"ğŸš€ Starting scraper for: {url}")
    
    slug = get_slug_from_url(url)
    if not slug:
        print("âŒ Invalid URL")
        return

    metadata = fetch_novel_metadata(slug)
    if not metadata:
        print("âŒ Failed to fetch metadata")
        return

    print(f"ğŸ“– Found Novel: {metadata['title']}")

    novel_doc = {
        'title': metadata['title'],
        'description': metadata['description'],
        'cover': metadata['cover'],
        'author': author_name,
        'authorEmail': admin_email,
        'category': metadata['tags'][0] if metadata['tags'] else 'Ø¹Ø§Ù…',
        'tags': metadata['tags'],
        'status': metadata['status'],
        'sourceUrl': url,
        'lastChapterUpdate': datetime.now(),
        'createdAt': datetime.now()
    }

    existing_novel = novels_collection.find_one({'title': metadata['title'], 'authorEmail': admin_email})
    
    if existing_novel:
        novel_id = existing_novel['_id']
        novels_collection.update_one({'_id': novel_id}, {'$set': {
            'cover': metadata['cover'],
            'status': metadata['status'],
            'lastChapterUpdate': datetime.now()
        }})
        print(f"ğŸ”„ Updated existing novel ID: {novel_id}")
    else:
        result = novels_collection.insert_one({**novel_doc, 'chapters': [], 'views': 0})
        novel_id = result.inserted_id
        print(f"ğŸ†• Created new novel ID: {novel_id}")

    chapters_list = fetch_all_chapters_list(slug)
    print(f"ğŸ“š Found {len(chapters_list)} chapters.")

    current_novel = novels_collection.find_one({'_id': novel_id})
    existing_numbers = [c['number'] for c in current_novel.get('chapters', [])]

    for chap in chapters_list:
        num = chap['number']
        
        if num in existing_numbers:
            print(f"â© Skipping Chapter {num} (Already exists)")
            continue

        print(f"ğŸ“¥ Scraping Chapter {num}...")
        content = scrape_chapter_content(slug, num)
        
        if content:
            try:
                # Ø§Ù„Ø­ÙØ¸ ÙÙŠ Firebase (Ø§Ù„Ù…Ø­ØªÙˆÙ‰)
                doc_ref = firestore_db.collection('novels').document(str(novel_id)).collection('chapters').document(str(num))
                doc_ref.set({
                    'title': chap['title'],
                    'content': content,
                    'lastUpdated': firestore.SERVER_TIMESTAMP
                })

                # Ø§Ù„ØªØ­Ø¯ÙŠØ« ÙÙŠ MongoDB (Ø§Ù„Ù…ÙŠØªØ§ Ø¯Ø§ØªØ§)
                chapter_meta = {
                    'number': num,
                    'title': chap['title'],
                    'createdAt': datetime.now(),
                    'views': 0
                }
                
                novels_collection.update_one(
                    {'_id': novel_id},
                    {'$push': {'chapters': chapter_meta}}
                )
                print(f"âœ… Saved Chapter {num}")
                
                time.sleep(1) 
            except Exception as e:
                print(f"âŒ Firebase/Mongo Error Ch {num}: {e}")
                continue
        else:
            print(f"âš ï¸ Empty content for Chapter {num}")

    print("âœ¨ Scraping Task Completed!")

# ==========================================
# Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoints)
# ==========================================

@app.route('/', methods=['GET'])
def health_check():
    return "ZEUS Scraper Service is Running âš¡", 200

@app.route('/scrape', methods=['POST'])
def trigger_scrape():
    auth_header = request.headers.get('Authorization')
    if auth_header != API_SECRET:
        return jsonify({'message': 'Unauthorized'}), 401

    data = request.json
    url = data.get('url')
    admin_email = data.get('adminEmail')
    author_name = data.get('authorName', 'ZEUS Bot')

    if not url or 'rewayat.club' not in url:
        return jsonify({'message': 'Invalid URL. Must be from rewayat.club'}), 400

    thread = threading.Thread(target=background_worker, args=(url, admin_email, author_name))
    thread.daemon = True 
    thread.start()

    return jsonify({
        'message': 'ØªÙ… Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø³Ø­Ø¨ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©. Ø³ØªØ¸Ù‡Ø± Ø§Ù„ÙØµÙˆÙ„ ØªØ¨Ø§Ø¹Ø§Ù‹.',
        'status': 'started'
    }), 200

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
