import os
import json
import time
import threading
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore
from pymongo import MongoClient
import certifi
from datetime import datetime

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ==========================================
app = Flask(__name__)
CORS(app)

# Ù…ÙØªØ§Ø­ Ø³Ø±ÙŠ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø· (ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ React Native)
API_SECRET = os.environ.get('API_SECRET', 'Zeusndndjddnejdjdjdejekk29393838msmskxcm9239484jdndjdnddjj99292938338zeuslojdnejxxmejj82283849')

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================================

# 1. MongoDB Setup
MONGO_URI = os.environ.get('MONGODB_URI')
if MONGO_URI:
    try:
        mongo_client = MongoClient(MONGO_URI, tlsCAFile=certifi.where())
        mongo_db = mongo_client.get_database() # ÙŠØ³ØªØ®Ø¯Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø§Ø¨Ø·
        novels_collection = mongo_db['novels']
        print("âœ… Connected to MongoDB")
    except Exception as e:
        print(f"âŒ MongoDB Connection Error: {e}")
else:
    print("âš ï¸ MONGO_URI not found in env vars")

# 2. Firebase Setup
FIREBASE_KEY = os.environ.get('FIREBASE_SERVICE_ACCOUNT')
if FIREBASE_KEY:
    try:
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ù†Øµ JSON (ÙƒÙ…Ø§ ÙÙŠ Railway Variables)
        cred_dict = json.loads(FIREBASE_KEY)
        cred = credentials.Certificate(cred_dict)
        firebase_admin.initialize_app(cred)
        firestore_db = firestore.client()
        print("âœ… Connected to Firebase Firestore")
    except Exception as e:
        print(f"âŒ Firebase Connection Error: {e}")
else:
    print("âš ï¸ FIREBASE_KEY not found in env vars")

# ==========================================
# Ù…Ù†Ø·Ù‚ Ø§Ù„Ø³Ø­Ø¨ (Scraper Logic)
# ==========================================

def get_slug_from_url(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±Ù Ø§Ù„ÙØ±ÙŠØ¯ Ù„Ù„Ø±ÙˆØ§ÙŠØ© Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    # Example: https://rewayat.club/novel/the-beginning-after-the-end/
    # Slug: the-beginning-after-the-end
    parts = url.split('/novel/')
    if len(parts) > 1:
        return parts[1].strip('/').split('/')[0]
    return None

def fetch_novel_metadata(slug):
    """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø±ÙˆØ§ÙŠØ© Ù…Ù† API Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
    api_url = f"https://api.rewayat.club/api/novel/{slug}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(api_url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            return {
                'title': data.get('arabic', data.get('english', 'Unknown')),
                'description': data.get('about', ''),
                'cover': f"https://api.rewayat.club{data.get('poster_url', '')}" if data.get('poster_url') else '',
                'status': 'Ù…ÙƒØªÙ…Ù„Ø©' if data.get('get_novel_status') == 'Ù…ÙƒØªÙ…Ù„Ø©' else 'Ù…Ø³ØªÙ…Ø±Ø©',
                'tags': [g['arabic'] for g in data.get('genre', [])],
                'slug': slug
            }
    except Exception as e:
        print(f"Error fetching metadata: {e}")
    return None

def fetch_all_chapters_list(slug):
    """Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØµÙˆÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±Ù‚ÙŠÙ…"""
    chapters = []
    page = 1
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    while True:
        # API Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ±Ù‚ÙŠÙ…
        url = f"https://api.rewayat.club/api/chapters/{slug}/?ordering=number&page={page}"
        try:
            print(f"Fetching chapters list page {page}...")
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                break
            
            data = response.json()
            results = data.get('results', [])
            
            if not results:
                break
                
            for item in results:
                chapters.append({
                    'number': item.get('number'),
                    'title': item.get('title'),
                    'id': item.get('id')
                })
            
            if not data.get('next'):
                break
                
            page += 1
            time.sleep(0.5) # Ù…Ù‡Ù„Ø© Ø¨Ø³ÙŠØ·Ø©
            
        except Exception as e:
            print(f"Error fetching chapters list: {e}")
            break
            
    return chapters

def scrape_chapter_content(slug, chapter_num):
    """Ø³Ø­Ø¨ Ù†Øµ Ø§Ù„ÙØµÙ„ Ù…Ù† ØµÙØ­Ø© HTML"""
    url = f"https://rewayat.club/novel/{slug}/{chapter_num}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù†Øµ ÙÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
            # Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹: <div class="content-area"> Ø£Ùˆ <div class="v-card__text">
            content_div = soup.find('div', class_='content-area')
            
            if not content_div:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¯ÙŠÙ„Ø©: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ div ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ p ÙƒØ«ÙŠØ±Ø©
                content_div = soup.find('div', class_=lambda x: x and 'unselectable' in x)
            
            if content_div:
                paragraphs = content_div.find_all('p')
                text_content = "\n\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
                return text_content
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø©: Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ p
            paragraphs = soup.find_all('p')
            # ØªØµÙÙŠØ© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (Ù…Ø«Ù„ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…)
            clean_text = "\n\n".join([p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20])
            return clean_text
            
    except Exception as e:
        print(f"Error scraping chapter {chapter_num}: {e}")
    
    return None

def background_worker(url, admin_email, author_name):
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    print(f"ğŸš€ Starting scraper for: {url}")
    
    slug = get_slug_from_url(url)
    if not slug:
        print("âŒ Invalid URL")
        return

    # 1. Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
    metadata = fetch_novel_metadata(slug)
    if not metadata:
        print("âŒ Failed to fetch metadata")
        return

    print(f"ğŸ“– Found Novel: {metadata['title']}")

    # 2. Ø¥Ù†Ø´Ø§Ø¡/ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±ÙˆØ§ÙŠØ© ÙÙŠ MongoDB
    novel_doc = {
        'title': metadata['title'],
        'description': metadata['description'],
        'cover': metadata['cover'],
        'author': author_name,
        'authorEmail': admin_email,
        'category': metadata['tags'][0] if metadata['tags'] else 'Ø¹Ø§Ù…',
        'tags': metadata['tags'],
        'status': metadata['status'],
        'sourceUrl': url,
        'lastChapterUpdate': datetime.now(),
        'createdAt': datetime.now()
    }

    # Ø§Ù„Ø¨Ø­Ø« Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„ØªØ­Ø¯ÙŠØ«Ù‡Ø§
    existing_novel = novels_collection.find_one({'title': metadata['title'], 'authorEmail': admin_email})
    
    if existing_novel:
        novel_id = existing_novel['_id']
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ù‚ÙˆÙ„ ÙÙ‚Ø·
        novels_collection.update_one({'_id': novel_id}, {'$set': {
            'cover': metadata['cover'],
            'status': metadata['status'],
            'lastChapterUpdate': datetime.now()
        }})
        print(f"ğŸ”„ Updated existing novel ID: {novel_id}")
    else:
        result = novels_collection.insert_one({**novel_doc, 'chapters': [], 'views': 0})
        novel_id = result.inserted_id
        print(f"ğŸ†• Created new novel ID: {novel_id}")

    # 3. Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙØµÙˆÙ„
    chapters_list = fetch_all_chapters_list(slug)
    print(f"ğŸ“š Found {len(chapters_list)} chapters.")

    # 4. Ø³Ø­Ø¨ Ø§Ù„ÙØµÙˆÙ„ ÙˆØ­ÙØ¸Ù‡Ø§
    new_chapters_meta = [] # Ù„ØªØ­Ø¯ÙŠØ« MongoDB
    
    # Ù†Ø­Ø¶Ø± Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø­Ø§Ù„ÙŠØ§Ù‹ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    current_novel = novels_collection.find_one({'_id': novel_id})
    existing_numbers = [c['number'] for c in current_novel.get('chapters', [])]

    for chap in chapters_list:
        num = chap['number']
        
        # Ø¥Ø°Ø§ Ø§Ù„ÙØµÙ„ Ù…ÙˆØ¬ÙˆØ¯ØŒ ØªØ®Ø·Ø§Ù‡ (ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø²Ø§Ù„Ø© Ù‡Ø°Ø§ Ø§Ù„Ø´Ø±Ø· Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø­ØªÙˆÙ‰)
        if num in existing_numbers:
            print(f"â© Skipping Chapter {num} (Already exists)")
            continue

        print(f"ğŸ“¥ Scraping Chapter {num}...")
        content = scrape_chapter_content(slug, num)
        
        if content:
            # A. Ø§Ù„Ø­ÙØ¸ ÙÙŠ Firebase (Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ)
            try:
                doc_ref = firestore_db.collection('novels').document(str(novel_id)).collection('chapters').document(str(num))
                doc_ref.set({
                    'title': chap['title'],
                    'content': content,
                    'lastUpdated': firestore.SERVER_TIMESTAMP
                })
            except Exception as e:
                print(f"âŒ Firebase Error Ch {num}: {e}")
                continue

            # B. Ø§Ù„ØªØ¬Ù‡ÙŠØ² Ù„Ù€ MongoDB (Ø§Ù„Ù…ÙŠØªØ§ Ø¯Ø§ØªØ§)
            chapter_meta = {
                'number': num,
                'title': chap['title'],
                'createdAt': datetime.now(),
                'views': 0
            }
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù€ Mongo Ù…Ø¨Ø§Ø´Ø±Ø© Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙÙˆØ±Ø§Ù‹
            novels_collection.update_one(
                {'_id': novel_id},
                {'$push': {'chapters': chapter_meta}}
            )
            print(f"âœ… Saved Chapter {num}")
            
            time.sleep(1) # Ø§Ø­ØªØ±Ø§Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        else:
            print(f"âš ï¸ Empty content for Chapter {num}")

    print("âœ¨ Scraping Task Completed!")

# ==========================================
# Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoints)
# ==========================================

@app.route('/', methods=['GET'])
def health_check():
    return "ZEUS Scraper Service is Running âš¡", 200

@app.route('/scrape', methods=['POST'])
def trigger_scrape():
    # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø³Ø±ÙŠ
    auth_header = request.headers.get('Authorization')
    if auth_header != API_SECRET:
        return jsonify({'message': 'Unauthorized'}), 401

    data = request.json
    url = data.get('url')
    admin_email = data.get('adminEmail')
    author_name = data.get('authorName', 'ZEUS Bot')

    if not url or 'rewayat.club' not in url:
        return jsonify({'message': 'Invalid URL. Must be from rewayat.club'}), 400

    # 2. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³Ø­Ø¨ ÙÙŠ Ø¹Ù…Ù„ÙŠØ© Ù…Ù†ÙØµÙ„Ø© (Thread)
    thread = threading.Thread(target=background_worker, args=(url, admin_email, author_name))
    thread.daemon = True # Ù„ØªØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
    thread.start()

    return jsonify({
        'message': 'ØªÙ… Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø³Ø­Ø¨ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©. Ø³ØªØ¸Ù‡Ø± Ø§Ù„ÙØµÙˆÙ„ ØªØ¨Ø§Ø¹Ø§Ù‹.',
        'status': 'started'
    }), 200

if __name__ == "__main__":
    from datetime import datetime
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
