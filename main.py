import os
import json
import time
import threading
import requests
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urlparse

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ==========================================
app = Flask(__name__)
CORS(app)

# Ù…ÙØªØ§Ø­ Ø³Ø±ÙŠ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø·
API_SECRET = os.environ.get('API_SECRET', 'Zeusndndjddnejdjdjdejekk29393838msmskxcm9239484jdndjdnddjj99292938338zeuslojdnejxxmejj82283849')

# Ø±Ø§Ø¨Ø· Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Node.js)
NODE_BACKEND_URL = os.environ.get('NODE_BACKEND_URL', 'https://c-production-3db6.up.railway.app')

# ==========================================
# Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© (Shared Scraper Tools)
# ==========================================

def get_headers(referer=None):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3',
    }
    if referer:
        headers['Referer'] = referer
    return headers

def fix_image_url(url, base_url='https://api.rewayat.club'):
    if not url: return ""
    if url.startswith('//'):
        return 'https:' + url
    elif url.startswith('/'):
        return base_url + url
    elif not url.startswith('http'):
        return base_url + '/' + url
    return url

def send_data_to_backend(payload):
    """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø§Ø¯Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    try:
        endpoint = f"{NODE_BACKEND_URL}/api/scraper/receive"
        headers = { 'Content-Type': 'application/json', 'Authorization': API_SECRET, 'x-api-secret': API_SECRET }
        response = requests.post(endpoint, json=payload, headers=headers, timeout=60)
        return response.status_code == 200
    except Exception as e:
        print(f"âŒ Failed to send data: {e}")
        return False

def check_existing_chapters(title):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø¨Ø§Ùƒ Ø¥Ù†Ø¯ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±"""
    try:
        endpoint = f"{NODE_BACKEND_URL}/api/scraper/check-chapters"
        headers = { 'Content-Type': 'application/json', 'Authorization': API_SECRET, 'x-api-secret': API_SECRET }
        response = requests.post(endpoint, json={'title': title}, headers=headers, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('exists'):
                return data['chapters'] # ÙŠØ¹ÙŠØ¯ Ù…ØµÙÙˆÙØ© Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…
            else:
                return []
        return []
    except Exception as e:
        print(f"âŒ Error checking existence: {e}")
        return []

# ==========================================
# ğŸŸ£ 1. Rewayat Club (Nuxt) Logic - Probe Mode
# ==========================================

def extract_from_nuxt(soup):
    try:
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'window.__NUXT__' in script.string:
                content = script.string
                match = re.search(r'poster_url:"(.*?)"', content)
                if not match: match = re.search(r'poster:"(.*?)"', content)
                if match:
                    raw_url = match.group(1)
                    return raw_url.encode('utf-8').decode('unicode_escape')
    except: pass
    return None

def fetch_metadata_rewayat(url):
    try:
        response = requests.get(url, headers=get_headers(), timeout=15)
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title_tag = soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else "Unknown Title"
        
        cover_url = extract_from_nuxt(soup) or ""
        if not cover_url:
            og_image = soup.find("meta", property="og:image")
            if og_image: cover_url = og_image["content"]
        cover_url = fix_image_url(cover_url)

        desc_div = soup.find(class_='text-pre-line') or soup.find('div', class_='v-card__text')
        description = desc_div.get_text(strip=True) if desc_div else ""
        
        return {
            'title': title, 'description': description, 'cover': cover_url,
            'status': "Ù…Ø³ØªÙ…Ø±Ø©", 'category': "Ø¹Ø§Ù…", 'tags': []
        }
    except Exception as e:
        print(f"Error rewayat metadata: {e}")
        return None

def scrape_chapter_rewayat(novel_url, chapter_num):
    url = f"{novel_url.rstrip('/')}/{chapter_num}"
    try:
        response = requests.get(url, headers=get_headers(), timeout=10)
        if response.status_code != 200: return None, None
        soup = BeautifulSoup(response.content, 'html.parser')
        paragraphs = soup.find_all('p')
        clean_paragraphs = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
        if clean_paragraphs:
            text = "\n\n".join(clean_paragraphs)
        else:
            div = soup.find('div', class_='pre-formatted') or soup.find('div', class_='v-card__text')
            text = div.get_text(separator="\n\n", strip=True) if div else ""
        
        if len(text.strip()) < 2: return None, None
        
        title_tag = soup.find(class_='v-card__subtitle') or soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else f"Ø§Ù„ÙØµÙ„ {chapter_num}"
        title = re.sub(r'^\d+\s*-\s*', '', title)
        return title, text
    except: return None, None

def worker_rewayat_probe(url, admin_email, metadata):
    existing_chapters = check_existing_chapters(metadata['title'])
    skip_meta = len(existing_chapters) > 0
    
    if not skip_meta:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': [], 'skipMetadataUpdate': False})

    current_chapter = 1
    errors = 0
    batch = []
    
    while current_chapter < 5000 and errors < 15:
        if current_chapter in existing_chapters:
            current_chapter += 1
            errors = 0
            continue
            
        chap_title, content = scrape_chapter_rewayat(url, current_chapter)
        if content:
            errors = 0
            batch.append({'number': current_chapter, 'title': chap_title, 'content': content})
            print(f"Fetched Ch {current_chapter}")
            if len(batch) >= 5:
                send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})
                batch = []
                time.sleep(1)
        else:
            errors += 1
            print(f"Failed Ch {current_chapter} ({errors}/15)")
        current_chapter += 1
        
    if batch:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})

# ==========================================
# ğŸŸ¢ 2. Madara Themes (Ar-Novel & Markaz Riwayat) - List Mode
# ==========================================

def get_base_url(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ + Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†)"""
    parsed = urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}"

def fetch_metadata_madara(url):
    try:
        response = requests.get(url, headers=get_headers(), timeout=15)
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        title_tag = soup.find(class_='post-title')
        title = title_tag.find('h1').get_text(strip=True) if title_tag else "Unknown"
        title = re.sub(r'\s*~.*$', '', title) 

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØºÙ„Ø§Ù
        cover = ""
        img_container = soup.find(class_='summary_image')
        if img_container:
            img_tag = img_container.find('img')
            if img_tag:
                cover = img_tag.get('src') or img_tag.get('data-src') or img_tag.get('srcset', '').split(' ')[0]
        
        if not cover:
            og_img = soup.find("meta", property="og:image")
            if og_img: cover = og_img["content"]

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ID Ø§Ù„Ø±ÙˆØ§ÙŠØ© (Ù‡Ø§Ù… Ù„Ø·Ù„Ø¨ AJAX)
        novel_id = None
        shortlink = soup.find("link", rel="shortlink")
        if shortlink:
            match = re.search(r'p=(\d+)', shortlink.get('href', ''))
            if match: novel_id = match.group(1)
            
        if not novel_id:
            id_input = soup.find('input', class_='rating-post-id')
            if id_input: novel_id = id_input.get('value')
            
        if not novel_id:
            body_class = soup.find('body').get('class', [])
            for c in body_class:
                if c.startswith('manga-id-'):
                    novel_id = c.replace('manga-id-', '')

        print(f"Found Novel ID: {novel_id}")

        # Ø§Ù„ÙˆØµÙ
        desc_div = soup.find(class_='summary__content') or soup.find(class_='description-summary')
        description = desc_div.get_text(separator="\n", strip=True) if desc_div else ""

        # Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
        genres_content = soup.find(class_='genres-content')
        category = "Ø¹Ø§Ù…"
        tags = []
        if genres_content:
            links = genres_content.find_all('a')
            tags = [a.get_text(strip=True) for a in links]
            if tags: category = tags[0]

        return {
            'title': title, 'description': description, 'cover': cover,
            'status': 'Ù…Ø³ØªÙ…Ø±Ø©', 'category': category, 'tags': tags,
            'novel_id': novel_id
        }
    except Exception as e:
        print(f"Error Madara Meta: {e}")
        return None

def fetch_metadata_markaz(url):
    """Ù…Ø®ØµØµ Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø±ÙˆØ§ÙŠØ§Øª Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¯Ù‚Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§ØªÙ‡Ù…"""
    return fetch_metadata_madara(url) # Ø§Ù„Ù‡ÙŠÙƒÙ„ Ù…Ø·Ø§Ø¨Ù‚ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ Ù„Ù‚Ø§Ù„Ø¨ Ù…Ø§Ø¯Ø§Ø±Ø§ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ

def parse_madara_chapters_from_html(soup):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØµÙˆÙ„ Ù…Ù† ÙƒÙˆØ¯ HTML"""
    chapters = []
    items = soup.find_all('li', class_='wp-manga-chapter')
    
    for item in items:
        a = item.find('a')
        if a:
            link = a.get('href')
            raw_title = a.get_text(strip=True)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ù‚Ù…
            num_match = re.search(r'(\d+)', raw_title)
            number = int(num_match.group(1)) if num_match else 0
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
            clean_title = re.sub(r'^\d+\s*[-â€“]\s*', '', raw_title).strip()
            
            if number > 0:
                chapters.append({'number': number, 'url': link, 'title': clean_title})
    
    return chapters

def fetch_chapter_list_madara(novel_id, novel_url):
    """Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙØµÙˆÙ„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø£ÙŠ Ù…ÙˆÙ‚Ø¹ Ù…Ø§Ø¯Ø§Ø±Ø§)"""
    chapters = []
    base_url = get_base_url(novel_url)
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© 1: AJAX Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠ Ù„Ù…Ø§Ø¯Ø§Ø±Ø§
    if novel_url:
        ajax_endpoint = f"{novel_url.rstrip('/')}/ajax/chapters/"
        try:
            headers = get_headers()
            headers['X-Requested-With'] = 'XMLHttpRequest'
            res = requests.post(ajax_endpoint, headers=headers, timeout=20)
            if res.status_code == 200:
                soup = BeautifulSoup(res.content, 'html.parser')
                chapters = parse_madara_chapters_from_html(soup)
                print(f"âœ… Chapters fetched via /ajax/chapters/ ({len(chapters)})")
        except Exception as e:
            print(f"AJAX endpoint failed: {e}")

    # Ù…Ø­Ø§ÙˆÙ„Ø© 2: admin-ajax.php (Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù…Ø§Ø¯Ø§Ø±Ø§)
    if not chapters and novel_id:
        try:
            # Ù‡Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ§Ù‹ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ØªØ«Ø¨ÙŠØªÙ‡
            admin_ajax_url = f"{base_url}/wp-admin/admin-ajax.php"
            data = {'action': 'manga_get_chapters', 'manga': novel_id}
            print(f"ğŸ”„ Trying admin-ajax at: {admin_ajax_url}")
            res = requests.post(admin_ajax_url, data=data, headers=get_headers(novel_url), timeout=20)
            if res.status_code == 200:
                soup = BeautifulSoup(res.content, 'html.parser')
                chapters = parse_madara_chapters_from_html(soup)
                print(f"âœ… Chapters fetched via admin-ajax ({len(chapters)})")
        except Exception as e:
            print(f"admin-ajax failed: {e}")
            
    # ØªØ±ØªÙŠØ¨ Ø§Ù„ÙØµÙˆÙ„
    if chapters:
        chapters.sort(key=lambda x: x['number'])
    
    return chapters

def scrape_chapter_madara(url):
    try:
        res = requests.get(url, headers=get_headers(), timeout=15)
        if res.status_code != 200: return None
        soup = BeautifulSoup(res.content, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù…Ø§Ø¯Ø§Ø±Ø§ + Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù…Ø±ÙƒØ² Ø§Ù„Ø±ÙˆØ§ÙŠØ§Øª
        container = soup.find(class_='reader-target') or \
                    soup.find(class_='reading-content') or \
                    soup.find(class_='text-left') or \
                    soup.find(class_='text-right') or \
                    soup.find(class_='entry-content')
            
        if container:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
            for bad in container.find_all(['div', 'script', 'style', 'input', 'ins', 'iframe', 'button']):
                # Ø¥Ø²Ø§Ù„Ø© Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªÙ†Ù‚Ù„ Ø£Ùˆ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù†Ø§Øª
                if bad.get('class') and any(c in ['nav-links', 'code-block', 'adsbygoogle', 'pf-ad', 'wpmcr-under-title-row'] for c in bad.get('class')):
                    bad.decompose()
                # Ø¥Ø²Ø§Ù„Ø© Ø²Ø± Ø§Ù„Ù‚Ø§Ø±Ø¦ Ø§Ù„Ø¹Ø§Ø¦Ù…
                if bad.get('id') == 'reader-btn':
                    bad.decompose()
            
            # Ø¥Ø²Ø§Ù„Ø© Ø­Ø§ÙˆÙŠØ§Øª Ø§Ù„ØªÙ†Ù‚Ù„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
            for nav in container.find_all('div', class_='nav-links'):
                nav.decompose()

            text = container.get_text(separator="\n\n", strip=True)
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = text.replace('Ø§ÙƒÙ…Ø§Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©', '')
            text = text.replace('Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©', '') # ØªÙ†Ø¸ÙŠÙ Ù†Øµ Ø§Ù„Ø²Ø± Ø¥Ø°Ø§ Ø¨Ù‚ÙŠ
            return text
        return None
    except: return None

def worker_madara_list(url, admin_email, metadata):
    existing_chapters = check_existing_chapters(metadata['title'])
    skip_meta = len(existing_chapters) > 0
    
    if not skip_meta:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': [], 'skipMetadataUpdate': False})

    all_chapters = fetch_chapter_list_madara(metadata.get('novel_id'), url)
    
    if not all_chapters:
        print(f"No chapters found for {metadata['title']}")
        return

    print(f"Processing {len(all_chapters)} chapters (Sorted Ascending).")
    
    batch = []
    for chap in all_chapters:
        if chap['number'] in existing_chapters:
            continue
            
        print(f"Scraping {metadata['title']} - Ch {chap['number']}...")
        content = scrape_chapter_madara(chap['url'])
        
        if content:
            batch.append({
                'number': chap['number'],
                'title': chap['title'],
                'content': content
            })
            
            if len(batch) >= 5:
                send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})
                batch = []
                time.sleep(1.2) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        
    if batch:
        send_data_to_backend({'adminEmail': admin_email, 'novelData': metadata, 'chapters': batch, 'skipMetadataUpdate': skip_meta})

# ==========================================
# Main Orchestrator
# ==========================================

@app.route('/', methods=['GET'])
def health_check():
    return "ZEUS Scraper Service is Running", 200

@app.route('/scrape', methods=['POST'])
def trigger_scrape():
    auth_header = request.headers.get('Authorization')
    if auth_header != API_SECRET: return jsonify({'message': 'Unauthorized'}), 401

    data = request.json
    url = data.get('url', '')
    admin_email = data.get('adminEmail')
    
    if not url: return jsonify({'message': 'No URL'}), 400

    # ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ø·Ù„Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
    if 'rewayat.club' in url:
        meta = fetch_metadata_rewayat(url)
        if not meta: return jsonify({'message': 'Failed metadata'}), 400
        thread = threading.Thread(target=worker_rewayat_probe, args=(url, admin_email, meta))
        thread.daemon = False 
        thread.start()
        return jsonify({'message': 'Scraping started (Rewayat Club).'}), 200
        
    elif 'ar-no.com' in url:
        meta = fetch_metadata_madara(url)
        if not meta: return jsonify({'message': 'Failed metadata'}), 400
        thread = threading.Thread(target=worker_madara_list, args=(url, admin_email, meta))
        thread.daemon = False
        thread.start()
        return jsonify({'message': 'Scraping started (Ar-Novel).'}), 200

    elif 'markazriwayat.com' in url:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù†Ø·Ù‚ Ù…Ø±ÙƒØ² Ø§Ù„Ø±ÙˆØ§ÙŠØ§Øª (ÙˆÙ‡Ùˆ Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Madara Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø·ÙÙŠÙØ©)
        meta = fetch_metadata_markaz(url)
        if not meta: return jsonify({'message': 'Failed metadata'}), 400
        thread = threading.Thread(target=worker_madara_list, args=(url, admin_email, meta))
        thread.daemon = False
        thread.start()
        return jsonify({'message': 'Scraping started (Markaz Riwayat).'}), 200

    else:
        return jsonify({'message': 'Unsupported Domain'}), 400

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, threaded=True)